# TrustVar: A Dynamic Framework for Trustworthiness Evaluation and Task Variation Analysis in Large Language Models

[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Akhmetov-VA/TrustVar)

## Project Description

**TrustVar** is a framework built on our previous LLM trustworthiness testing system. While we previously focused on how LLMs handle tasks, we now rethink the evaluation procedure itself. TrustVar shifts the focus: we investigate the quality of tasks themselves, not just model behavior.

### Key Innovation

Unlike traditional frameworks that test models through tasks, TrustVar tests tasks through models. We analyze tasks as research objects, measuring their ambiguity, sensitivity, and structure, then examine how these parameters influence model behavior.

### Core Features

- **Task Variation Generation**: Automatically creates families of task reformulations
- **Model Robustness Testing**: Evaluates model stability under formulation changes
- **Task Sensitivity Index (TSI)**: Measures how strongly formulations affect model success
- **Multi-language Support**: English and Russian tasks with extensible architecture
- **Interactive Pipeline**: Unified system for data loading, task generation, variation, model evaluation, and visual analysis

## Table of Contents

- [Project Architecture](#project-architecture)
- [Project Structure](#project-structure)
- [Quick Start](#quick-start)
- [Metrics](#metrics)

## Project Architecture

![TrustVar Architecture](assets/TrustVar_Pipeline.jpg)

### Core Components

- **Data Ingestion** - accepts preformatted datasets in CSV, JSON, Excel, and Parquet formats, supporting both user uploads and built-in collections like SLAVA, RuBia, etc;
- **Task Generator** - applies five controlled transformations: lexico-syntactic paraphrasing, length variation, stylistic shifts, synonym substitution, and word reordering to create semantically equivalent variants;
- **Perturbation Settings** - sets up each transformation with user-configurable parameters (10 by default);
- **Task Pool** - serves as a persistent repository organizing tasks by six trustworthiness dimensions (truthfulness, safety, fairness, robustness, privacy, ethics) and maintaining evaluation queues;
- **LLM Tester** - executes inference on both local models via Ollama and remote APIs, recording outputs with complete metadata for reproducibility;
- **Analyzer** - measures response stability using coefficient of variation, feeding instability flags back for task refinement;
- **Task Meta-Evaluator** - computes the Task Sensitivity Index (TSI) across all model-task pairs, flagging high-TSI items for revision;
- **Evaluator & Visualizer** - computes RtAR, TFNR, Accuracy, and Pearson correlation metrics;
- **Dashboard and Leaderboard** - combine Metrics with Analyser data and display the results for user convenience


## Project Structure

```
trustvar
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”œâ”€â”€ routes
â”‚   â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”‚   â””â”€â”€ tasks.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â”œâ”€â”€ constants.py
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”‚   â”œâ”€â”€ services
â”‚   â”‚   â”‚   â”œâ”€â”€ ab_test_analyzer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ eval_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ include_exclude_evaluator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ judge_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ model_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rta_evaluator.py
â”‚   â”‚   â”‚   â””â”€â”€ task_service.py
â”‚   â”‚   â”œâ”€â”€ tasks
â”‚   â”‚   â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”‚   â”‚   â”œâ”€â”€ health_check_task.py
â”‚   â”‚   â”‚   â”œâ”€â”€ inference_task.py
â”‚   â”‚   â”‚   â”œâ”€â”€ model_download_task.py
â”‚   â”œâ”€â”€ ui
â”‚   â”‚   â”œâ”€â”€ components
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset_uploader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ datasets_section.py
â”‚   â”‚   â”‚   â”œâ”€â”€ general_section.py
â”‚   â”‚   â”‚   â”œâ”€â”€ models_section.py
â”‚   â”‚   â”‚   â”œâ”€â”€ results_section.py
â”‚   â”‚   â”‚   â”œâ”€â”€ spider_chart_variations.py
â”‚   â”‚   â”‚   â”œâ”€â”€ task_monitor.py
â”‚   â”‚   â”‚   â””â”€â”€ tasks_section.py
â”‚   â”‚   â”œâ”€â”€ api_client.py
â”‚   â”‚   â””â”€â”€ app.py
â”œâ”€â”€ Dockerfile.celery
â”œâ”€â”€ Dockerfile.streamlit
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.dev.yml
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
```

## Quick Start

### Requirements

- **Docker** and **Docker Compose**
- **Python 3.11+** (for local development)

### Launch with Docker

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd trustvar
   ```

2. **Create `.env` file with environment variables:**
   ```env
    HF_TOKEN=<your-hf-token>
    OPENAI_API_KEY=<your-openai-key>
    OPENAI_BASE_URL=<base_url>
    FRONTEND_PORT=27364
   ```

3. **Launch all services:**
   ```bash
   docker-compose up -d
   ```

4. **Download datasets and auxiliary information:**
   
   After running `docker-compose up`, you need to download the datasets and auxiliary information from our Google Drive and upload them to MongoDB:
   
   **[ðŸ“¥ Download Datasets](https://drive.google.com/drive/folders/1jvBWvAc9JcjLYQ8T09xoDKUkwjCf7tiI?usp=sharing)**
   
   The drive contains:
   - **Accuracy_Groups.json** - Accuracy metrics grouped by categories
   - **Accuracy.json** - Main accuracy dataset
   - **Correlation.json** - Correlation metrics
   - **IncludeExclude.json** - Include/Exclude analysis data
   - **RtAR.json** - Refuse to Answer metrics
   - **TFNR.json** - True False Negative Rate metrics
   - **jailbreak.json** - Jailbreak detection tasks
   - **ood_detection.json** - Out-of-distribution detection
   - **privacy_assessment.json** - Privacy assessment tasks
   - **stereotypes_detection_3.json** - Stereotype detection
   - **tasks.json** - Task definitions
   - And many more specialized datasets...


5. **Open the web interface:**
   - Monitoring: http://localhost:27366
   - MongoDB Express: http://localhost:27374

## Metrics

Supported metric types:

- **Accuracy** - Response accuracy
- **RtA (Refuse to Answer)** - Analysis of answer refusals
- **Correlation** - Correlation with reference answers
- **Include/Exclude** - Analysis of element inclusion/exclusion

## License

This project is licensed under the MIT License.